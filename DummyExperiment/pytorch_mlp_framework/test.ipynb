{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building module with input shape  torch.Size([2, 5, 4])\n",
      "shape after 1st layer:  torch.Size([2, 5, 3])\n",
      "shape after final layer:  torch.Size([2, 5, 4])\n",
      "Building module with input shape  torch.Size([2, 5, 5])\n",
      "shape after 1st layer:  torch.Size([2, 5, 5])\n",
      "shape after final layer:  torch.Size([2, 5, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x4 and 5x5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayer_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeural_Memory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mforward_inference(keys, queries, values)\n\u001b[1;32m     17\u001b[0m     diff \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m-\u001b[39moutput\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(diff\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Desktop/data_science/arrhythmia/MLP_CW/DummyExperiment/pytorch_mlp_framework/titan.py:72\u001b[0m, in \u001b[0;36mNeuralMemory.forward_inference\u001b[0;34m(self, k, q, v)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_inference\u001b[39m(\u001b[38;5;28mself\u001b[39m, k, q, v):\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    Used to train the model with post attention sequence\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    k: keys (input)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    q: query\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    v: values (target)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     momentary_surprise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurprise_score(k, v)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Initialize accumulated_surprise if it's the first call\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulated_surprise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/data_science/arrhythmia/MLP_CW/DummyExperiment/pytorch_mlp_framework/titan.py:48\u001b[0m, in \u001b[0;36mNeuralMemory.surprise_score\u001b[0;34m(self, k, v)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msurprise_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, k, v):\n\u001b[0;32m---> 48\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(k)\n\u001b[1;32m     49\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(prediction, v)\n\u001b[1;32m     51\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params()\n",
      "File \u001b[0;32m~/Desktop/data_science/arrhythmia/MLP_CW/DummyExperiment/pytorch_mlp_framework/titan.py:39\u001b[0m, in \u001b[0;36mNeuralMemory.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 39\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput_Layer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m     40\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mforward(out))\n\u001b[1;32m     41\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput_Layer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mforward(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x4 and 5x5)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from titan import NeuralMemory\n",
    "\n",
    "# create dummy keys, queries and values\n",
    "keys = torch.randn(2, 5, 4)\n",
    "queries = keys\n",
    "values = torch.randn(2, 5, 4)\n",
    "\n",
    "\n",
    "# create a NeuralMemory instance\n",
    "memory = NeuralMemory(input_shape=keys.shape, hidden_units=3)\n",
    "\n",
    "output=None\n",
    "for i in range(5):\n",
    "    output = memory.forward_inference(keys, queries, values)\n",
    "    diff = values-output\n",
    "    print(diff.mean().item())\n",
    "\n",
    "print(\"-----------------------\")\n",
    "for i in range(5):\n",
    "    output = memory.forward_inference(keys, queries, values)\n",
    "    memory.apply_cached_updates()\n",
    "    diff = values-output\n",
    "    print(diff.mean().item())\n",
    "\n",
    "output = memory.forward_inference(keys, queries, values)\n",
    "\n",
    "print(\"input shapes: \", keys.shape, queries.shape, values.shape)\n",
    "print(\"output shape: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building module with input shape  torch.Size([2, 5, 5])\n",
      "shape after 1st layer:  torch.Size([2, 5, 5])\n",
      "shape after final layer:  torch.Size([2, 5, 5])\n",
      "Loss: 1.167250394821167\n",
      "layer_dict.Queries.weight grad mean: 0.000503\n",
      "layer_dict.Queries.bias grad mean: 0.034973\n",
      "layer_dict.Keys.weight grad mean: 0.000000\n",
      "layer_dict.Keys.bias grad mean: 0.003391\n",
      "layer_dict.Values.weight grad mean: 0.000000\n",
      "layer_dict.Values.bias grad mean: 0.000671\n",
      "layer_dict.Neural_Memory.layer_dict.Input_Layer.weight grad mean: 0.000000\n",
      "layer_dict.Neural_Memory.layer_dict.Input_Layer.bias grad mean: 0.000000\n",
      "layer_dict.Neural_Memory.layer_dict.bn0.weight grad mean: 0.000000\n",
      "layer_dict.Neural_Memory.layer_dict.bn0.bias grad mean: 0.000000\n",
      "layer_dict.Neural_Memory.layer_dict.Output_Layer.weight grad mean: 0.000000\n",
      "layer_dict.Neural_Memory.layer_dict.Output_Layer.bias grad mean: 0.000000\n",
      "layer_dict.Output_Layer.weight grad mean: -0.014919\n",
      "layer_dict.Output_Layer.bias grad mean: 0.067435\n",
      "------------------------\n",
      "Loss: 1.6122071743011475\n",
      "layer_dict.Queries.weight grad mean: 0.001842\n",
      "layer_dict.Queries.bias grad mean: 0.038854\n",
      "layer_dict.Keys.weight grad mean: 0.000000\n",
      "layer_dict.Keys.bias grad mean: 0.003391\n",
      "layer_dict.Values.weight grad mean: 0.000000\n",
      "layer_dict.Values.bias grad mean: 0.000671\n",
      "layer_dict.Neural_Memory.layer_dict.Input_Layer.weight grad mean: 0.000000\n",
      "layer_dict.Neural_Memory.layer_dict.Input_Layer.bias grad mean: 0.000000\n",
      "layer_dict.Neural_Memory.layer_dict.bn0.weight grad mean: 0.000000\n",
      "layer_dict.Neural_Memory.layer_dict.bn0.bias grad mean: 0.000000\n",
      "layer_dict.Neural_Memory.layer_dict.Output_Layer.weight grad mean: 0.000000\n",
      "layer_dict.Neural_Memory.layer_dict.Output_Layer.bias grad mean: 0.000000\n",
      "layer_dict.Output_Layer.weight grad mean: 0.016906\n",
      "layer_dict.Output_Layer.bias grad mean: -0.083041\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from titan import TryBackprop, NeuralMemory\n",
    "\n",
    "# Create a random input tensor with shape (batch_size, seq_len, feature_dim)\n",
    "x = torch.randn(2, 5, 4)\n",
    "model = TryBackprop(input_shape=x.shape, hidden_units=5)\n",
    "\n",
    "# Register a hook to zero out outer-loss gradients for Neural Memory parameters.\n",
    "for param in model.layer_dict[\"Neural_Memory\"].parameters():\n",
    "    param.register_hook(lambda grad: torch.zeros_like(grad))\n",
    "\n",
    "output = model(x)\n",
    "target = torch.randn_like(output)\n",
    "loss = F.mse_loss(output, target)\n",
    "loss.backward()\n",
    "model.layer_dict[\"Neural_Memory\"].apply_cached_updates()\n",
    "model.layer_dict[\"Neural_Memory\"].reset_computational_history()\n",
    "\n",
    "# Print out the loss and one example of a parameter's gradient\n",
    "print(\"Loss:\", loss.item())\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad mean: {param.grad.mean().item():.6f}\")\n",
    "    else:\n",
    "        print(f\"{name} has no gradient\")\n",
    "\n",
    "\n",
    "x2 = torch.randn(2, 5, 4)\n",
    "target2 = torch.randn_like(output)\n",
    "output2 = model(x2)\n",
    "loss = F.mse_loss(output2, target2)\n",
    "loss.backward()\n",
    "model.layer_dict[\"Neural_Memory\"].apply_cached_updates()\n",
    "model.layer_dict[\"Neural_Memory\"].reset_computational_history()\n",
    "print(\"------------------------\")\n",
    "# Print out the loss and one example of a parameter's gradient\n",
    "print(\"Loss:\", loss.item())\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad mean: {param.grad.mean().item():.6f}\")\n",
    "    else:\n",
    "        print(f\"{name} has no gradient\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from titan import NeuralMemory, TryBackprop\n",
    "\n",
    "# create dummy keys, queries and values\n",
    "keys = torch.randn(2, 5, 4)\n",
    "queries = keys\n",
    "values = torch.randn(2, 5, 4)\n",
    "\n",
    "\n",
    "# create a NeuralMemory instance\n",
    "model = TryBackprop(input_shape=keys.shape, hidden_units=3)\n",
    "\n",
    "output=None\n",
    "for i in range(5):\n",
    "    output = model.layer_dict[\"Neural_Memory\"].forward_inference(keys, queries, values)\n",
    "    model.layer_dict[\"Neural_Memory\"].apply_cached_updates()\n",
    "    diff = values-output\n",
    "    print(diff.mean().item())\n",
    "\n",
    "print(\"-----------------------\")\n",
    "for i in range(5):\n",
    "    output = model.layer_dict[\"Neural_Memory\"].forward_inference(keys, queries, values)\n",
    "    model.layer_dict[\"Neural_Memory\"].apply_cached_updates()\n",
    "    diff = values-output\n",
    "    print(diff.mean().item())\n",
    "\n",
    "output = memory.forward_inference(keys, queries, values)\n",
    "\n",
    "print(\"input shapes: \", keys.shape, queries.shape, values.shape)\n",
    "print(\"output shape: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(32, 100, 17) \n",
    "y = torch.randn(32, 100, 17)\n",
    "\n",
    "b = torch.bmm(x, y.transpose(2, 1))\n",
    "\n",
    "print(b.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
